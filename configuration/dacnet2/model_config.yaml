# attn:
#   layers:
#     - 1
#     - 2
#     - 3
#   units:
#     - 4
#     - 8
#     - 16
#     - 32
#     - 64
#   dropout:
#     - 0.0
#     - 0.1
#     - 0.2
#   batch_size:
#     - 32
#   lr:
#     - 0.003
#   type: pytorch

# cnn:
#   layers:
#     - 1
#     - 2
#     - 3
#   units:
#     - 4
#     - 8
#     - 16
#     - 32
#     - 64
#   dropout:
#     - 0.0
#     - 0.1
#     - 0.2
#   batch_size:
#     - 32
#   lr:
#     - 0.003
#   type: pytorch

# lstm:
#   keep_dims: False
#   layers:
#     - 1
#     - 2
#     - 3
#   units:
#     - 4
#     - 8
#     - 16
#     - 32
#     - 64
#   dropout:
#     - 0.0
#     - 0.1
#     - 0.2
#   batch_size:
#     - 32
#   lr:
#     - 0.003
#   type: pytorch

# itransformer:
#   layers:
#     - 1
#     - 2
#     - 3
#   units:
#     - 4
#     - 8
#     - 16
#     - 32
#     - 64
#   dropout:
#     - 0.0
#     - 0.1
#     - 0.2
#   output_attention: false
#   n_heads:
#     - 2
#   use_norm: true
#   batch_size:
#     - 32
#   lr:
#     - 0.003
#   type: pytorch


dacnet:
  layers:
    - 1
    - 2
    - 3
  units:
    - 4
    - 8
    - 16
    - 32
    - 64
  dropout:
    - 0.0
    - 0.1
    - 0.2
  batch_size:
    - 32
  lr:
    - 0.003
  binarize_scores:
    - true
    - false
  body_block:
    - fc
    - lstm
    - cnn
  scorer:
    - linear
    - attn
  detach_parent: false
  type: pytorch

dacnetattn:
  layers:
    - 1
    - 2
    - 3
  units:
    - 4
    - 8
    - 16
    - 32
    - 64
  dropout:
    - 0.0
    - 0.1
    - 0.2
  batch_size:
    - 32
  lr:
    - 0.003
  binarize_scores:
    - true
    - false
  body_block:
    - lstm
    - cnn
  head_block:
    - attn
  scorer:
    - linear
    - attn
  detach_parent: false
  type: pytorch


dense:
  keep_dims: False
  layers:
    - 1
    - 2
    - 3
  units:
    - 4
    - 8
    - 16
    - 32
    - 64
  dropout:
    - 0.0
    - 0.1
    - 0.2
  batch_size:
    - 32
  lr:
    - 0.003
  type: pytorch
